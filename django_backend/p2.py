# -*- coding: utf-8 -*-
"""p2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P0qrBW6zloWZDaVZYDwcfabLfEheT29O
"""

from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM,AutoTokenizer
import re

model=AutoModelForCausalLM.from_pretrained("/content/drive/MyDrive/LegalGPT-ft-Model",device_map="auto") #same path where you stored Model
tokenizer=AutoTokenizer.from_pretrained("/content/drive/MyDrive/LegalGPT-ft-Tokenizer") #same path where you stored Tokenizer

def generate_response(comment):
  instructions_string = f"""LegalGPT, functioning as a virtual legal consultant, communicates in clear, accessible language, escalating to technical depth upon request. \
  It reacts to feedback aptly and ends responses with its signature 'â€“LegalGPT'. \
  LegalGPT will tailor the length of its responses to match the user's query, providing concise acknowledgments to brief expressions of gratitude or feedback, \
  thus keeping the interaction natural and engaging.

  Please respond to the following question.
  """

  prompt_template = lambda comment: f'''[INST] {instructions_string} \n{comment} \n[/INST]'''

  prompt = prompt_template(comment)

  model.eval()

  inputs = tokenizer(prompt, return_tensors="pt")

  outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=280)

  generated_text=tokenizer.batch_decode(outputs)[0]

  pattern = r"\[/INST\](.*?)</s>"
  match = re.search(pattern, generated_text, re.DOTALL)

  if match:
      extracted_text = match.group(1).strip()
  else:
      extracted_text = "Pattern not found in the generated text."

  return extracted_text